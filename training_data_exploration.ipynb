{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "# MODEL EVALUATION\n",
    "# text embeddings, establish basline performance\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140\n",
    "### Source Authors\n",
    "Sentiment140 was created by Alec Go, Richa Bhayani, and Lei Huang, who were Computer Science graduate students at Stanford University.\n",
    "\n",
    "[Data Link](http://help.sentiment140.com/for-students)\n",
    "\n",
    "### Source Purpose\n",
    "\"Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search. This is described in our paper.\"\n",
    "\n",
    "### Schema\n",
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "0. the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "1. the id of the tweet (2087)\n",
    "2. the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "3. the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "4. the user that tweeted (robotickilldozr)\n",
    "5. the text of the tweet (Lyx is cool)\n",
    "### Project Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment140 Train Data Shape: (1600000, 6)\n",
      "Negative Oakland Tweets\n",
      "@CodaQueen oh wait he does have 1 in Oakland on the 18th. Can't understand why he only has 1 &amp; in Oakland \n",
      "\n",
      "dear morrissey, stop cancelling shows. it bums people out. first ft lauderdale, now oakland.  get it together. thanks &lt;3\n",
      "\n",
      "Just worked my last night at the Oakland Children's and I'm sad.  That was my 2nd home for 4 years. New hospital tomorrow...woo\n",
      "\n",
      "Positive Oakland Tweets\n",
      "@MadameSoybean Ah, I think that you think that I'm in Oakland. But I'm 145 miles north of there. So you didn't pass by. I was confused. \n",
      "\n",
      "@AsPgameLive  i miss nick swisher as well and follow him- i happy for him but want him to come home to oakland  sniff sniff\n",
      "\n",
      "@debraoakland  Only telling the truth Dibster \n",
      "\n",
      "Negative Foodbank Tweets\n",
      "@KGMB9 i wish the Hawaii Food Bank's food drive wasn't always so close to the Letter Carrier's Food Drive (May 9) \n",
      "\n",
      "@JMC_Ministries Churches aren't involved as we r called 2b thru Christ. If we were, there wouldn't b need 4 shelters/food banks. \n",
      "\n",
      "@mrskutcher  Where I live, all the small markets deliver their day-old bread to the food bank, but times are really tough, even for me \n",
      "\n",
      "Positive Foodbank Tweets\n",
      "Donate to the Hawaii Food Bank TODAY!! \n",
      "\n",
      "okay going to sleep now have a 7 AM wakeup call. Meeting my classmates to work on our project. Anyone works for a food bank in LA? @ me \n",
      "\n",
      "Ok off vacation  Cfc girls remember we are re filling our local food banks. Bring yours to wordshop wed.\n",
      "\n",
      "Negative LGBT Tweets\n",
      "i feel like im the only lgbt in this room right now that does not have a faghag with with me...  *feels left out* \n",
      "\n",
      "lgbtnetwork: Thanks for the mention, but we never get anything from them  that's the price for saying it how you see it http://ow.ly/64pm\n",
      "\n",
      "@godisvoid  that is horrible. I was in an lgbta org for a few years, &amp; i cant believe the horrid arguments against gay marriage ive heard\n",
      "\n",
      "Positive LGBT Tweets\n",
      "@choiceusa, @aclulgbt, @saveroe, @bitchmagazine, Follow IWN for updates from your allies in Idaho! \n",
      "\n",
      "@LincolnukLGBT  Wow!  Thanks!!  \n",
      "\n",
      "Follow my hubby @CommissionerAnt - Law Professor, LGBT Rights Activist, Politician, and all around amazing guy #samesexsunday \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment140\n",
    "# Data Cleaning\n",
    "sentiment_train_data = pd.read_csv('./data/Sentiment140/train_data.csv', header=None, \n",
    "                         names=['polarity', 'id', 'date', 'query', 'user', 'text'], encoding='latin')\n",
    "# Data Size\n",
    "print(f'Sentiment140 Train Data Shape: {sentiment_train_data.shape}')\n",
    "\n",
    "def search_text(text_series, search_pattern, case):\n",
    "    return text_series.str.contains(search_pattern, case=case)\n",
    "\n",
    "def print_tweets(data, polarity, n):\n",
    "    for elem in data[data.polarity == polarity].text:\n",
    "        if n <= 0:\n",
    "            break\n",
    "        else:\n",
    "            n-=1\n",
    "        print(elem)\n",
    "        print()\n",
    "\n",
    "# Baseline performance\n",
    "oakland_results = sentiment_train_data[search_text(sentiment_train_data.text, 'oakland', case=False)]\n",
    "print('Negative Oakland Tweets')\n",
    "print_tweets(oakland_results, 0, 3)\n",
    "print('Positive Oakland Tweets')\n",
    "print_tweets(oakland_results, 4, 3)\n",
    "foodbank_results = sentiment_train_data[search_text(sentiment_train_data.text, 'food bank', case=False)]\n",
    "print('Negative Foodbank Tweets')\n",
    "print_tweets(foodbank_results, 0, 3)\n",
    "print('Positive Foodbank Tweets')\n",
    "print_tweets(foodbank_results, 4, 3)\n",
    "lgbt_results = sentiment_train_data[search_text(sentiment_train_data.text, 'lgbt', case=False)]\n",
    "print('Negative LGBT Tweets')\n",
    "print_tweets(lgbt_results, 0, 3)\n",
    "print('Positive LGBT Tweets')\n",
    "print_tweets(lgbt_results, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASU Twitter Graph\n",
    "### Source Authors\n",
    "R. Zafarani and H. Liu, (2009). Social Computing Data Repository at ASU [http://socialcomputing.asu.edu]. Tempe, AZ: Arizona State University, School of Computing, Informatics and Decision Systems Engineering.\n",
    "\n",
    "[Data Link](http://socialcomputing.asu.edu/datasets/Twitter)\n",
    "\n",
    "### Source Purpose\n",
    "Social Computing Data Repository hosts data from a collection of many different social media sites, most of which have blogging capacity. Some of the prominent social media sites included in this repository are BlogCatalog, Twitter, MyBlogLog, Digg, StumbleUpon, del.icio.us, MySpace, LiveJournal, The Unofficial Apple Weblog (TUAW), Reddit, etc. The repository contains various facets of blog data including blog site metadata like, user defined tags, predefined categories, blog site description; blog post level metadata like, user defined tags, date and time of posting; blog posts; blog post mood (which is defined as the blogger's emotions when (s)he wrote the blog post); blogger name; blog post comments; and blogger social network.\n",
    "\n",
    "The repository has been designed in 2009 by Reza Zafarani and Huan Liu. Funding support from the Air Force Office of Scientific Research (AFOSR) and Office of Naval Research (ONR) is gratefully acknowledged. The credit also goes to our dataset creaters who made gathering this repository possible.\n",
    "\n",
    "### Schema\n",
    "nodes.csv\n",
    "-- it's the file of all the users. This file works as a dictionary of all the users in this data set. It's useful for fast reference. It contains\n",
    "all the node ids used in the dataset\n",
    "\n",
    "edges.csv\n",
    "-- this is the friendship/followership network among the users. The friends/followers are represented using edges. Edges are directed.\n",
    "Here is an example.\n",
    "\n",
    "{1,2}\n",
    "\n",
    "This means user with id \"1\" is followering user with id \"2\".\n",
    "\n",
    "Attribute Information:\n",
    "Twitter is a social news website. It can be viewed as a hybrid of email, instant messaging and sms messaging all rolled into one neat and simple package. It's a new and easy way to discover the latest news related to subjects you care about.\n",
    "\n",
    "- Basic statistics\n",
    "Number of Nodes: 11,316,811\n",
    "Number of Edges: 85,331,846\n",
    "\n",
    "### Project Relevance\n",
    "* While this has no actual text information, this data set can be very useful if augmented with specific values we want to prioritize in our PageRank implementation\n",
    "* Good testing ground for scaling up algorithms due to data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 585094\n",
      "Largest number of connections: 5545\n",
      "Nodes with largest in-degrees: {5994113: 5545, 7496: 3568, 1349110: 3443, 3493: 2187, 3402: 1993}\n",
      "Built-in PageRank: {5994113: 0.003860802208591317, 7496: 0.0024850155981141118, 1349110: 0.002373434560173397, 3493: 0.001272556362991607, 3402: 0.0011883608812889813}\n"
     ]
    }
   ],
   "source": [
    "# ASU Twitter Graph\n",
    "# graph imports\n",
    "import operator\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Data Cleaning\n",
    "twitter_nodes = pd.read_csv('./data/Twitter-dataset/data/nodes.csv', header=None)\n",
    "twitter_edges = pd.read_csv('./data/Twitter-dataset/data/edges.csv', header=None)\n",
    "# let's take a random sample of twitter_edges to make it a bit smaller\n",
    "sampled_twitter_edges = twitter_edges.sample(frac=0.01)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(sampled_twitter_edges.values)\n",
    "print(f'Number of nodes: {len(list(G.nodes))}')\n",
    "in_degrees = dict(G.in_degree)\n",
    "print(f'Largest number of connections: {max(in_degrees.values())}')\n",
    "\n",
    "# Baseline performance\n",
    "print(f'Nodes with largest in-degrees: {dict(sorted(in_degrees.items(), key=operator.itemgetter(1), reverse=True)[:5])}')\n",
    "pr = nx.pagerank(G)\n",
    "print(f'Built-in PageRank: {dict(sorted(pr.items(), key=operator.itemgetter(1), reverse=True)[:5])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheng-Caverlee-Lee Twitter Scrape\n",
    "### Source Authors\n",
    "Z. Cheng, J. Caverlee, and K. Lee. You Are Where You Tweet: A Content-Based Approach to Geo-locating Twitter Users. In Proceeding of the 19th ACM Conference on Information and Knowledge Management (CIKM), Toronto, Oct 2010. (Bibtex)\n",
    "\n",
    "[Data Link](https://archive.org/details/twitter_cikm_2010)\n",
    "\n",
    "### Source Purpose\n",
    "This dataset is a collection of scraped public twitter updates used in coordination with an academic project to study the geolocation data related to twittering. \n",
    "\n",
    "### Schema\n",
    "The training set contains 115,886 Twitter users and 3,844,612 updates from the users. All the locations of the users are self-labeled in United States in city-level granularity. The test set contains 5,136 Twitter users and 5,156,047 tweets from the users. All the locations of users are uploaded from their smart phones with the form of \"UT: Latitude,Longitude\".\n",
    "\n",
    "* “training set users.txt” - “UserID\\tUserLocation”.\n",
    "* “training set tweets.txt” - “UserID\\tTweetID\\tTweet\\tCreatedAt”.\n",
    "\n",
    "### Project Relevance\n",
    "* Geographic location imputation\n",
    "* Size and diversity of tweet text makes this a good candidate for a development test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 classes support: Los Angeles        0.038987\n",
      "New York           0.037382\n",
      "Los Angeles, CA    0.026336\n",
      "Chicago            0.023696\n",
      "New York, NY       0.021124\n",
      "Name: 1, dtype: float64\n",
      "User data size: (115886, 2)\n",
      "Tweets data size: (3679161, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cheng-Caverlee-Lee Twitter Scrape\n",
    "# Data Cleaning\n",
    "# This data set comes in a tsv file with inconsistent line lengths and tabs embedded in some tweet texts\n",
    "# Base pandas has difficulty reading in the data, some cleaning required\n",
    "def read_ccl_data(fp):\n",
    "    \"\"\"Read in the tsv values accomodating for line length inconsistency and embedded tabs in text values\"\"\"\n",
    "    ccl_data_dict = dict()\n",
    "    with open(fp) as f:\n",
    "        # counter represents the \"row\" number of the data\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            payload = line.split('\\t')\n",
    "            \n",
    "            # Handle embedded tabs within tweet text\n",
    "            if len(payload) > 4:\n",
    "                first_col = payload[0]\n",
    "                second_col = payload[1]\n",
    "                fourth_col = payload[-1]\n",
    "                # Non-ideal notation but it works for now\n",
    "                third_col = ' '.join(payload[-2:1:-1][::-1])\n",
    "                payload = [first_col, second_col, third_col, fourth_col]\n",
    "            \n",
    "            # Skip rows with missing data values\n",
    "            # TODO: Use data schema patterns to identify which values are missing and impute or mark None\n",
    "            if len(payload) < 4:\n",
    "                #skip the line for now\n",
    "                continue\n",
    "                \n",
    "            ccl_data_dict[counter] = payload\n",
    "            counter += 1\n",
    "\n",
    "    return pd.DataFrame(ccl_data_dict).T\n",
    "\n",
    "def query_for_hand_labeling(data, search_term, nrows, outfp):\n",
    "    text_search = data[search_text(data.loc[:, 2], search_term, case=False)]\n",
    "    text_search.to_csv(outfp)\n",
    "\n",
    "\n",
    "def read_hand_labeled_data(fp):\n",
    "    df = pd.read_csv(fp)\n",
    "    new = df.loc[:, '3'].str.split(',', expand=True)\n",
    "    new.loc[:, 2] = new.loc[:, 2].str.strip()\n",
    "    new = new.rename({0: 'date', 1: 'political', 2: 'sentiment'}, axis=1)\n",
    "    df = df.drop(['3', '4', '5'], axis=1).rename({'Unnamed: 0':'user_id', \n",
    "                                                  '0':'tweet_id', '1':'tweet_id2', \n",
    "                                                  '2':'tweet'}, axis=1)\n",
    "    return df.join(new).astype({'political': 'int32', 'sentiment': 'int32'})\n",
    "\n",
    "ccl_train_tweets = read_ccl_data('./data/twitter_cikm_2010/training_set_tweets.txt')\n",
    "ccl_train_users = pd.read_csv('./data/twitter_cikm_2010/training_set_users.txt', sep='\\t', header=None)\n",
    "\n",
    "# Data Size\n",
    "print(f'User data size: {ccl_train_users.shape}')\n",
    "print(f'Tweets data size: {ccl_train_tweets.shape}')\n",
    "\n",
    "# Class balance\n",
    "print(f'Top 5 classes support:\\n {(ccl_train_users.loc[:, 1].value_counts() / ccl_train_users.shape[0]).head()}')\n",
    "\n",
    "# Hand labeled sentiment and political\n",
    "#query_for_hand_labeling(ccl_train_tweets, 'lgbt', 100, './data/twitter_cikm_2010/jdunn_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support for Political Class 1: 0.76\n",
      "Support for Political Class 0: 0.24\n",
      "Support for Sentiment Class 1: 0.35\n",
      "Support for Sentiment Class 0: 0.58\n",
      "Support for Sentiment Class -1: 0.07\n"
     ]
    }
   ],
   "source": [
    "# supports\n",
    "df = read_hand_labeled_data('./data/twitter_cikm_2010/jdunn_labeled.csv')\n",
    "print(f'Support for Political Class 1: {df[df.political == 1].shape[0]/df.shape[0]}')\n",
    "print(f'Support for Political Class 0: {df[df.political == 0].shape[0]/df.shape[0]}')\n",
    "\n",
    "print(f'Support for Sentiment Class 1: {df[df.sentiment == 1].shape[0]/df.shape[0]}')\n",
    "print(f'Support for Sentiment Class 0: {df[df.sentiment == 0].shape[0]/df.shape[0]}')\n",
    "print(f'Support for Sentiment Class -1: {df[df.sentiment == -1].shape[0]/df.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
